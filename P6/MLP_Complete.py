import numpy as np

class MLP_Complete:
    """
    Constructor: Computes MLP.

    Args:
        inputLayer (int): size of input
        hiddenLayer (int): size of hidden layer.
        outputLayer (int): size of output layer
        seed (scalar): seed of the random numeric.
        epislon (scalar) : random initialization range. e.j: 1 = [-1..1], 2 = [-2,2]...
    """
    def __init__(self, inputLayer, hiddenLayers, outputLayer, seed=0, epsilon=0.12):
        np.random.seed(seed)
        # Filas (salidas) x Columnas (entradas)
        # Array de matrices de pesos
        self.thetas = []
        # Capa de entrada
        self.thetas.append(np.random.uniform(-epsilon,epsilon, (hiddenLayers[0], inputLayer + 1)))
        # Capas ocultas 
        for i in range(1, len(hiddenLayers)):
            self.thetas.append(np.random.uniform(-epsilon,epsilon, (hiddenLayers[i], hiddenLayers[i-1] + 1)))
        # Capa de salida    
        self.thetas.append(np.random.uniform(-epsilon,epsilon, (outputLayer, hiddenLayers[len(hiddenLayers) - 1] + 1)))
        
    """
    Reset the thetas matrices created in the constructor by thetas matrices manualy loaded.

    Args:
        thetas (array_like): Weights for the first layer in the neural network.
    """
    def new_trained(self,thetas):
        self.thetas = thetas
        
    """
    Num elements in the training data. (private)

    Args:
        x (array_like): input data. 
    """
    def _size(self,x):
        return x.shape[0]
    
    """
    Computes de sigmoid function of z (private)

    Args:
        z (array_like): activation signal received by the layer.
    """
    def _sigmoid(self,z_wb):
        return 1 / (1 + np.exp(-z_wb))
    
    """
    Computes de sigmoid derivation of de activation (private)

    Args:
        a (array_like): activation received by the layer.
    """   
    def _sigmoidPrime(self,a):
        return a * (1 - a)

    """
    Run the feedfordward neural network step

    Args:
        x (array_like): input of the neural network.

	Return 
	------
	a1,a2,a3 (array_like): activation functions of each layers
    z2,z3 (array_like): signal fuction of two last layers
    """
    def feedforward(self,x):
        # Capa de entrada
        a1 = x
        ai = []
        ai.append(a1)
        zi = []
        
        # Capas ocultas
        for i in range(0, len(self.thetas)):
            ones = np.ones((self._size(ai[i]), 1))
            ai[i] = np.hstack([ones, ai[i]])
            zi.append(np.dot(ai[i], self.thetas[i].T))
            ai.append(self._sigmoid(zi[i]))

        # Capa de salida
        zi.append(np.dot(ai[len(self.thetas) - 1], self.thetas[len(self.thetas) - 1].T))
        ai.append (self._sigmoid(zi[len(zi) - 1]))

        # Devolvemos las activaciones 
        # y los valores sin ejecutar la función de activación
        return a1,ai,zi
    
    """
    Computes only the cost of a previously generated output (private)

    Args:
        yPrime (array_like): output generated by neural network.
        y (array_like): output from the dataset
        lambda_ (scalar): regularization parameter

	Return 
	------
	J (scalar): the cost.
    """
    def compute_cost(self, yPrime, y, lambda_):
        m = len(y)
        J = (-1 / m) * np.sum(y * np.log(yPrime) + (1 - y) * np.log(1 - yPrime))
        return J + self._regularizationL2Cost(m,lambda_)
    
    """
    Get the class with highest activation value

    Args:
        a3 (array_like): output generated by neural network.

	Return 
	------
	p (scalar): the class index with the highest activation value.
    """
    def binaryMap(self, x):
        if ((1.0 - x)<0.5):
            return 1.0
        else:
            return 0.0
    
    def predict(self,a3):
        if (len(a3[0])>1):
            p = np.argmax(a3, axis=1)
        else:
            p = np.array([[ self.binaryMap(x)  for  x in row] for row in a3])
        return p

    """
    Compute the gradients of both theta matrix parámeters and cost J

    Args:
        x (array_like): input of the neural network.
        y (array_like): output of the neural network.
        lambda_ (scalar): regularization.

	Return 
	------
	J: cost
    grad1, grad2: the gradient matrix (same shape than theta1 and theta2)
    """
    def compute_gradients(self, x, y, lambda_):
        J = 0
        a1,ai,zi = self.feedforward(x)
        m = self._size(a1)

        # Capa de Salida
        yPrime = ai[len(ai) - 1]
        J = self.compute_cost(yPrime,y,lambda_)
        
        # Creamos una lista de deltas
        # donde deltaOut es el error de la capa de salida
        deltas = []
        deltaOut = np.subtract(yPrime,y)
        deltas.append(deltaOut)

        # Creamos una lista de gradientes
        grads = []

        # Capas Ocultas
        numHiddenLayers = len(self.thetas) - 1
        for i in range(numHiddenLayers, 0, -1):
            thetaNoBias = np.delete(self.thetas[i], [0], axis=1)
            aiNoBias = np.delete(ai[i], [0], axis=1)
            aiPrime = self._sigmoidPrime(aiNoBias)

            deltas.append(np.dot(thetaNoBias.T, deltas[numHiddenLayers - i].T).T * aiPrime)
            grads.append(np.dot(deltas[numHiddenLayers - i].T, ai[i]) / m + self._regularizationL2Gradient(self.thetas[i],lambda_,m))

        # No hace falta calcular delta1 porque la capa de entrada no puede tener error
        
        # Gradiente de la capa de entrada
        ones = np.ones((self._size(a1), 1))
        a1WithBias = np.hstack([ones, a1])
        grads.append(np.dot(deltas[numHiddenLayers].T, a1WithBias) / m  + self._regularizationL2Gradient(self.thetas[0],lambda_,m))

        return (J, grads)
    
    """
    Compute L2 regularization gradient

    Args:
        theta (array_like): a theta matrix to calculate the regularization.
        lambda_ (scalar): regularization.
        m (scalar): the size of the X examples.

	Return 
	------
	L2 Gradient value
    """
    def _regularizationL2Gradient(self,theta,lambda_,m):
        thetaNoBias = theta[:, 1:]
        L2 = (lambda_ / m) * thetaNoBias
        zeros = np.zeros((self._size(thetaNoBias), 1))
        L2 = np.hstack([zeros, L2])
        return L2
    
    """
    Compute L2 regularization cost

    Args:
        lambda_ (scalar): regularization.
        m (scalar): the size of the X examples.

	Return 
	------
	L2 cost value
    """
    def _regularizationL2Cost(self, m, lambda_):
        L2 = 0
        for i in range(len(self.thetas)):
            L2 += np.sum(np.square(self.thetas[i][:, 1:]))
        L2 = (lambda_ / (2 * m)) * L2
        return L2
    
    """
    Run the backpropagation algorithm

    Args:
        x (array_like): input of the neural network.
        y (array_like): output of the neural network.
        alpha (scalar): learning rate.
        lambda_ (scalar): regularization.
        numIte (scalar): number of iterations.
        verbose (scalar): print the cost every verbose iterations.

    Return
    ------
    Jhistory (array_like): cost history.
    """
    def backpropagation(self, x, y, alpha, lambda_, numIte, verbose=0):
        Jhistory = []
        for i in range(numIte):
            (J, grads) = self.compute_gradients(x, y, lambda_)

            for j in range(len(self.thetas)):
                self.thetas[j] -= alpha * grads[len(self.thetas) - 1 - j]

            Jhistory.append(J)

            if verbose > 0 :
                if i % verbose == 0 or i == (numIte-1):
                    print(f"Iteration {(i+1):6}: Cost {float(J):8.4f}   ")
        
        return Jhistory
    
"""
target_gradient function of gradient test 1
"""
def target_gradient(input_layer_size,hidden_layer_size,num_labels,x,y,reg_param):
    hiddenLayer = [hidden_layer_size]
    mlp = MLP_Complete(input_layer_size,hiddenLayer,num_labels)
    J, grads = mlp.compute_gradients(x,y,reg_param)
    return J, grads, mlp.thetas

"""
costNN function of gradient of test 1
"""
def costNN(Theta1, Theta2, x, ys, reg_param):
    mlp = MLP_Complete(x.shape[1],[1], ys.shape[1])
    thetas = [Theta1,Theta2]
    mlp.new_trained(thetas)
    J, grads = mlp.compute_gradients(x,ys,reg_param)
    return J, grads

"""
mlp_backprop_predict to be executed in test 2
"""
def MLP_backprop_predict(X_train, y_train, X_test, alpha, lambda_, num_ite, verbose):
    mlp = MLP_Complete(X_train.shape[1],[25],y_train.shape[1])
    Jhistory = mlp.backpropagation(X_train,y_train,alpha,lambda_,num_ite,verbose)
    a1,ai,zi = mlp.feedforward(X_test)
    y_pred = mlp.predict(ai[len(ai) - 1])
    return y_pred
